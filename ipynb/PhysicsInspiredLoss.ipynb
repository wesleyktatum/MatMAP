{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations of loss during the training of networks can use weighting to reinforce the learning of specific variables and features in data. In order to reinforce physically reasonable predictions, you can impose more strict penalties for predictions in physically invalid predictions. For example, a prediction of Power Conversion Efficiency (PCE) above the Schockley-Quassier predicted maximum PCE could increase exponentially, rather than linearly.\n",
    "\n",
    "The classes developed below are wrappers for PyTorch tensors loss functions, which additionally modify these classes using theoretically and emprically derived boundaries for network loss calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366, 11)\n"
     ]
    }
   ],
   "source": [
    "#Take in data as a dataframe for easy preprocessing\n",
    "device_df = pd.read_excel('/Users/wesleytatum/Desktop/OPV_device_df.xlsx')\n",
    "print (device_df.shape)\n",
    "device_df.head()\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 100\n",
    "batch_size = int(len(device_df['PCE'])*0.8*0.9) # 90% of x_train samples\n",
    "learning_rate = 0.0008\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X = device_df[['Time (min)', 'Temp (C)']] #input features used to make prediction\n",
    "Y = device_df[['PCE', 'VocL', 'Jsc', 'FF']] #target features to be predicted\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, shuffle = True) #split dataset into separate testing and training datasets\n",
    "\n",
    "pce_train = y_train['PCE']\n",
    "pce_test = y_test['PCE']\n",
    "\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train.values.astype(np.float32)) #convert pd.DataFrame -> np.ndarray -> torch.tensor\n",
    "pce_train_tensor = torch.tensor(pce_train.values.astype(np.float32))\n",
    "\n",
    "\n",
    "#create tensor with features and targets\n",
    "train_tensor = torch.utils.data.TensorDataset(x_train_tensor, pce_train_tensor)\n",
    "#create iterable dataset with batches\n",
    "training_data_set = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test.values.astype(np.float32))\n",
    "pce_test_tensor = torch.tensor(pce_test.values.astype(np.float32))\n",
    "\n",
    "\n",
    "test_tensor = torch.utils.data.TensorDataset(x_test_tensor, pce_test_tensor)\n",
    "testing_data_set = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the neural network\n",
    "class NN1(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        super(NN1, self).__init__()\n",
    "        \n",
    "        #emedding layer\n",
    "        self.em_layer = nn.Linear(in_dims, out_dims)\n",
    "        \n",
    "        #hidden layers\n",
    "        self.h_layer1 = nn.Linear(out_dims, 32)\n",
    "        self.h_layer2 = nn.Linear(32, 16)\n",
    "        self.h_layer3 = nn.Linear(16, 8)\n",
    "        \n",
    "        #output layers\n",
    "        self.PCE_branch = nn.Sequential(\n",
    "            nn.Dropout(p = 0.3),\n",
    "            nn.Linear(8, 32),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Dropout(p = 0.3),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #data enters embedding layer\n",
    "        out = self.em_layer(x)\n",
    "        \n",
    "        #embedded data is passed to hidden layers\n",
    "        out = self.h_layer1(out)\n",
    "        out = self.h_layer2(out)\n",
    "        out = self.h_layer3(out)\n",
    "        \n",
    "        #embedded data is passed to output layer\n",
    "        out = self.PCE_branch(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_data_set, criterion, optimizer):\n",
    "    train_epoch_loss = []\n",
    "    train_losses = []\n",
    "    \n",
    "    train_total = 0\n",
    "    \n",
    "    #switch model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    criterion = criterion\n",
    "    \n",
    "    #wrapper to create progress bar & results for each epoch's training\n",
    "    progress = tqdm_notebook(training_data_set, desc = 'Progress:', leave = True)\n",
    "    \n",
    "    for train_data, labels in progress:\n",
    "        \n",
    "        train_data = train_data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.zero_grad() #zero out any gradients from prior loops \n",
    "        out = model(train_data) #gather model predictions for this loop\n",
    "        \n",
    "        #calculate error in the predictions\n",
    "        loss = criterion(out, labels)\n",
    "\n",
    "        #BACKPROPOGATE LIKE A MF\n",
    "        loss.backward\n",
    "        optimizer.step()\n",
    "        \n",
    "        #send results to progress bar and save loss for this batch\n",
    "        progress.set_description(f'Loss: {loss.item()}:.3f')\n",
    "        train_losses.append(loss.item())\n",
    "        train_total+=1\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    #calculate and save total error for this epoch of training\n",
    "    epoch_loss = sum(train_losses)/train_total\n",
    "    train_epoch_loss.append(epoch_loss)\n",
    "    \n",
    "    train_epoch_loss.append(sum(train_losses)/train_total)\n",
    "    \n",
    "    #update progress bar\n",
    "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')\n",
    "    \n",
    "    return train_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, testing_data_set, criterion, optimizer):\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = criterion\n",
    "\n",
    "\n",
    "    #don't update nodes during evaluation b/c not training\n",
    "    with torch.no_grad():\n",
    "#         correct = 0\n",
    "        test_losses = []\n",
    "    \n",
    "        test_total = 0\n",
    "\n",
    "        for inputs, labels in testing_data_set:\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            out = model(inputs)\n",
    "\n",
    "    \n",
    "            # calculate loss per batch of testing data\n",
    "            test_loss = criterion(out, labels)\n",
    "            \n",
    "            test_losses.append(test_loss.item())\n",
    "            test_total += 1 \n",
    "            \n",
    "\n",
    "        test_epoch_loss = sum(test_losses)/test_total\n",
    "\n",
    "        print(f\"Total testing loss is: {test_epoch_loss}\")\n",
    "    return test_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCE_Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    This class contains loss functions that use a mean-squared-error loss for reasonable predictions.\n",
    "    They inherit from torch.nn.Module just like the custom model. For physically unreasonable conditions,\n",
    "    prediction loss is more severely calculated. What qualifies as reasonable is based on empirically\n",
    "    gathered datasets and literature reported boundaries of performance in P3HT:PCBM OPV devices.\n",
    "    \n",
    "    For the following Power Conversion Efficiency predictions that are improbable, the loss is penalized:\n",
    "    - PCE < 0%\n",
    "    - PCE > 6%\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PCE_Loss, self).__init__()\n",
    "\n",
    "    @staticmethod   \n",
    "    def forward(predictions, labels):\n",
    "#     def forward(predictions, labels):\n",
    "        print (predictions.size())\n",
    "        print (labels.size())\n",
    "        \n",
    "        result_list = torch.zeros(predictions.size(0))\n",
    "        \n",
    "        for x, y in zip(predictions, labels):\n",
    "            el_count = 0\n",
    "\n",
    "            if torch.le(x, torch.tensor([0])) == torch.tensor([1]):\n",
    "                #Exponential MSE for x <= 0\n",
    "                print(f\"prediction = {x}, lower threshold violated\")\n",
    "                error = torch.add(x, torch.neg(y))\n",
    "                element_result = torch.pow(error, 2)\n",
    "                element_result = torch.pow(element_result, 2)\n",
    "\n",
    "\n",
    "            elif torch.ge(x, torch.tensor([6])) == torch.tensor([1]):\n",
    "                #exponential MSE for x >= 6\n",
    "                print(f\"prediction = {x}, upper threshold violated\")\n",
    "                error = torch.add(x, torch.neg(y))\n",
    "                element_result = torch.pow(error, 2)\n",
    "                element_result = torch.pow(element_result, 2)\n",
    "\n",
    "            else:\n",
    "                print(f\"prediction = {x}\")\n",
    "                error = torch.add(x, torch.neg(y))\n",
    "                element_result = torch.pow(error, 2)\n",
    "                \n",
    "            result_list[el_count] = element_result\n",
    "            el_count+=1\n",
    "\n",
    "            result = result_list.mean()\n",
    "\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our beautiful NN model\n",
    "# takes in 2 features (anneal time, anneal temp) \n",
    "# predicts 4 metrics (PCE, Voc, Jsc, FF)\n",
    "model = NN1(in_dims = 2, out_dims = 4).to(device)\n",
    "\n",
    "#define the loss function and the optimizer\n",
    "criterion = PCE_Loss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49733d11694f408e99d6974b97a25ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress:', max=2, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([263, 1])\n",
      "torch.Size([263])\n",
      "prediction = tensor([-6.0054], grad_fn=<SelectBackward>), lower threshold violated\n",
      "torch.Size([29, 1])\n",
      "torch.Size([29])\n",
      "prediction = tensor([-5.1114], grad_fn=<SelectBackward>), lower threshold violated\n",
      "\n",
      "Epoch #1\tTrain Loss: 112.665\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0d319f3b73a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                     \u001b[0mtraining_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                     optimizer = optimizer)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    }
   ],
   "source": [
    "#empty list to hold loss per epoch\n",
    "train_epoch_losses = []\n",
    "\n",
    "\n",
    "test_epoch_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_epoch_loss, = train_model(model = model,\n",
    "                                    training_data_set = training_data_set,\n",
    "                                    criterion = criterion,\n",
    "                                    optimizer = optimizer)\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    \n",
    "    \n",
    "    test_epoch_loss = eval_model(model = model,\n",
    "                                 testing_data_set = testing_data_set,\n",
    "                                 criterion = criterion,\n",
    "                                 optimizer = optimizer)\n",
    "    test_epoch_losses.append(test_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ca37b9861f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'training error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_epoch_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'testing error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "epochs = np.arange(1, (num_epochs+1), 1)\n",
    "\n",
    "plt.plot(epochs, train_epoch_losses, c = 'k', label = 'training error')\n",
    "plt.plot(epochs, test_epoch_losses, c = 'r', label = 'testing error')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title(\"Total Training & Testing Error\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
